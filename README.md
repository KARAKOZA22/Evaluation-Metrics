# Evaluation metrics
<h2>Accuracy</h2>Calculating how many predicted instances are true over all the classes.
<h2>Confussion matrix</h2> 
Contains <b>TP and TN </b> which are the truely predicted values, <b>FN, and FP </b> which are the false predicted values like in <b>False Negative (FN)</b> , it is classified as Negative but this is false, it is positive.
<h2>Precision</h2>
It indicates how many values are truely predicted as positive from all predicted as positive
<h2>Recall</h2>
It indicates how many values are truely predicted as positive from all true labels as positive
<h2>F1-Score</h2>
It is the geometric mean between <b>Precision, and Recall</b><br><br>
<b>Geometric mean of [1,2,3,4] = (1*2*3*4)^(1/4)</b><br><br>
<img width="720" alt="image-29" src="https://github.com/KARAKOZA22/Metrics-/assets/96451039/ef4fab6c-6e3e-40dd-b270-46ebbc0edede">

<h2>ROC Curve</h2>
<b> ROC = TP / FP </b><br><br>
<img  height = "400"
src = "https://github.com/KARAKOZA22/Metrics-/assets/96451039/13546ff0-87e7-4eb8-a5bf-a9a972c3949e)" >


<h2>Summary</h2>

![1_fUnVMuu91SR_Ehrfvne2iA](https://github.com/KARAKOZA22/Metrics-/assets/96451039/ffb8b41d-75e0-4eea-92da-39c68d9166a8)

